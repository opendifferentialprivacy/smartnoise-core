\documentclass[11pt]{scrartcl} % Font size
\input{../structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{
	\normalfont\normalsize
	\textsc{Harvard Privacy Tools Project}\\ % Your university, school and/or department name(s)
	\vspace{25pt} % Whitespace
	\rule{\linewidth}{0.5pt}\\ % Thin top horizontal rule
	\vspace{20pt} % Whitespace
	{\huge Covariance Sensitivity Proofs}\\ % The assignment title
	\vspace{12pt} % Whitespace
	\rule{\linewidth}{2pt}\\ % Thick bottom horizontal rule
	\vspace{12pt} % Whitespace
}

% \author{\LARGE} % Your name

\date{\normalsize\today} % Today's date (\today) or a custom date

\begin{document}

\maketitle

\section{Preliminaries}

\begin{definition}
Let $X$ be a matrix of values and $X_i$ indicate the $i^{\text{th}}$ column of the matrix. Denote the sample mean of column $X_i$ as $\bar{X}_i$, and let $n$ be the size of $X_i$. Then the covariance matrix of $X$ has $ij^{\text{th}}$ element 
$$ \frac{1}{n-1} \sum_{k=1}^n (x_{ki} - \bar{X}_i)(x_{kj} - \bar{X}_j).$$
\end{definition}
\begin{lemma}
\label{lemma:cancel}
Let $X$ be a matrix of values and $X_i$ indicate the $i^{\text{th}}$ column of the matrix. Denote the sample mean of column $X_i$ as $\bar{X}_i$, and let $n$ be the size of $X_i$. Then,
$\forall i,$
$$ \sum_{j=1}^n (x_{ji} - \bar{X}_i) = 0.$$
\end{lemma}

\begin{proof}
\begin{align*}
\sum_{j=1}^n (x_{ji} - \bar{X}_i) &= \sum_{j=1}^n x_{ji} - n \bar{X}_i,\\
	&= \sum_{j=1}^n x_{ji} - n\left( \frac{1}{n}\sum_{j=1}^n x_{ji}\right), \\
	&= 0.
\end{align*}
\end{proof}

\begin{lemma}
\label{cov:rewrite}
Let $X$ be a matrix and let
$$ f_{ij}(X) = \sum_{k=1}^n (x_{ki} - \bar{X}_i)(x_{kj} - \bar{X}_j).$$
Note that this is equivalent to the $ij^{\text{th}}$ element of the sample covariance matrix for $X$, without the normalization by $n-1$. Consider the matrix $X'$ equal to $X$ with a single row $Y$ added, so that $X_i' = X_i \cup \{y_i\}$. Say $X_i$ has size $n$. Let  $\bar{X}_i$, $\bar{X}_j$, $\bar{X}_i'$, and $\bar{X}_j'$ be the sample means of $X_i, X_j, X_i'$ and $X_j'$ respectively. Then,
$$ f_{ij}(X') = f_{ij}(X) + n(\bar{X}_i - \bar{X}_i')(\bar{X}_j - \bar{X}_j') + (y_i - \bar{X}_i)(y_j - \bar{X}_j).$$
\end{lemma}

\begin{proof}
Note that
\begin{align*}
f_{ij}(X') &= \sum_{k=1}^{n+1} (x_{ki}' - \bar{X}_i')(x_{kj}' - \bar{X}_j'),\\
	&= \sum_{k=1}^{n} (x_{ki} - \bar{X}_i')(x_{kj} - \bar{X}_j') + (y_i - \bar{X}_i')(y_j - \bar{X}_j'),\\
	&= \sum_{k=1}^{n} \left( (x_{ki} - \bar{X}_i )+ (\bar{X}_i - \bar{X}_i' )\right) \left( (x_{kj} - \bar{X}_j )+ (\bar{X}_j - \bar{X}_j') \right) + (y_i - \bar{X}_i')(y_j - \bar{X}_j'),\\
	&= \sum_{k=1}^{n} (x_{ki} - \bar{X}_i)(x_{kj} - \bar{X}_j) + (\bar{X}_j - \bar{X}_j')\sum_{k=1}^{n} (x_{ki}-\bar{X}_i) + (\bar{X}_i - \bar{X}_i') \sum_{k=1}^{n} (x_{kj} - \bar{X}_j),\\
	& \hspace{1cm} + \sum_{k=1}^{n} (\bar{X}_i - \bar{X}_i')(\bar{X}_j - \bar{X}_j') + (y_i - \bar{X}_i')(y_j - \bar{X}_j'),\\
	&= f_{ij}(X) + n(\bar{X}_i - \bar{X}_i')(\bar{X}_j - \bar{X}_j') + (y_i - \bar{X}_i')(y_j - \bar{X}_j'),
\end{align*}
where the cancellation of the second and third terms in the second-to-last line is due to Lemma \ref{lemma:cancel}.
\end{proof}

\begin{lemma}
\label{lemma:term1bound}
Let $X$ be a matrix of values and $X_i$ indicate the $i^{\text{th}}$ column of the matrix. Let  $X_i$ have size $n$ and consider the matrix $X'$ equal to $X$ with a single row $Y$ added, so that $X_i' = X_i \cup \{y_i\}$. Say that the space of datapoints $\mathcal{X}_i$ that the elements of $X_i'$ are drawn from is bounded above by $M_i$ and bounded below by $m_i$. Let  $\bar{X}_i$, $\bar{X}_j$, $\bar{X}_i'$, and $\bar{X}_j'$ be the sample means of $X_i, X_j, X_i'$ and $X_j'$ respectively. Then,
$$ n \left\vert (\bar{X}_i - \bar{X}_i')(\bar{X}_j - \bar{X}_j') \right\vert \le \frac{n}{(n+1)^2}(M_i - m_i)(M_j - m_j).$$
\end{lemma}

\begin{proof}
Note that
\begin{align*}
 n \left\vert (\bar{X}_i - \bar{X}_i')(\bar{X}_j - \bar{X}_j') \right\vert &= n\left\vert \left( \frac{1}{n} \sum_{k=1}^n x_{ki} - \frac{1}{n+1} \sum_{k=1}^{n+1} x_{ki}' \right) \left( \frac{1}{n} \sum_{k=1}^n x_{kj} - \frac{1}{n+1} \sum_{k=1}^{n+1} x_{kj}' \right)\right\vert, \\
 	&= n \left\vert \left( \left(\frac{1}{n} - \frac{1}{n+1}\right) \sum_{k=1}^n x_{ki} - \frac{y_i}{n+1} \right)\left( \left(\frac{1}{n} - \frac{1}{n+1}\right) \sum_{k=1}^n x_{kj} - \frac{y_j}{n+1} \right) \right\vert, \\
	&= n  \left\vert \left(\frac{1}{n(n+1)} \sum_{k=1}^n x_{ki} - \frac{y_i}{n+1} \right)\left( \frac{1}{n(n+1)} \sum_{k=1}^n x_{kj} - \frac{y_j}{n+1} \right) \right\vert, \\
	&= \frac{n}{(n+1)^2} \left\vert \left( \frac{1}{n} \sum_{k=1}^n x_{ki} - \frac{y_i}{n+1} \right)\left( \frac{1}{n} \sum_{k=1}^n x_{kj} - \frac{y_j}{n+1} \right) \right\vert,\\
	&\le \frac{n}{(n+1)^2} (M_i - m_i)(M_j - m_j).
 \end{align*}
\end{proof}

\begin{lemma}
\label{lemma:term2bound}
Let $X$ be a matrix of values and $X_i$ indicate the $i^{\text{th}}$ column of the matrix. Let  $X_i$ have size $n$ and consider the matrix $X'$ equal to $X$ with a single row $Y$ added, so that $X_i' = X_i \cup \{y_i\}$. Say that the space of datapoints $\mathcal{X}_i$ that the elements of $X_i'$ are drawn from is bounded above by $M_i$ and bounded below by $m_i$. Let  $\bar{X}_i$, $\bar{X}_j$, $\bar{X}_i'$, and $\bar{X}_j'$ be the sample means of $X_i, X_j, X_i'$ and $X_j'$ respectively. Then,
$$ \left\vert (y_i - \bar{X}_i')(y_j - \bar{X}_j') \right\vert \le \frac{n^2}{(n+1)^2}(M_i - m_i)(M_j-m_j).$$
\end{lemma}

\begin{proof}
Note that
\begin{align*}
 \left\vert (y_i - \bar{X}_i')(y_j - \bar{X}_j') \right\vert &= \left\vert \left( y_i - \frac{y_i + n \bar{X}_i}{n+1}\right) \left( y_j - \frac{y_j + n \bar{X}_j}{n+1}\right) \right\vert, \\
 	&= \frac{1}{(n+1)^2} \left\vert \left((n+1)y_i - y_i - n\bar{X}_i \right)\left((n+1)y_j - y_j - n\bar{X}_j \right) \right\vert, \\
	&= \frac{n^2}{(n+1)^2} \left\vert (y_i - \bar{X}_i)(y_j - \bar{X}_j)\right\vert, \\
	&\le \frac{n^2}{(n+1)^2} (M_i - m_i)(M_j - m_j).
\end{align*}
\end{proof}

\section{Neighboring Definition: Add/Drop One}
\subsection{$\ell_1$-sensitivity}

\begin{theorem}
Let $X$ be a matrix of values and let $X_i$ indicate the $i^{\text{th}}$ column of the matrix. Let
$$ f_{ij} (X)= \sum_{k=1}^n (x_{ki} - \bar{X}_i)(x_{kj} - \bar{X}_j).$$
Say that the space of datapoints $\mathcal{X}_i$ that $X_i$ is drawn from is bounded above by $M_i$ and bounded below by $m_i$. Then the $\ell_1$-sensitivity in the add/drop-one model of $f(\cdot)$ is bounded above by
 $$ \frac{n}{(n+1)}  (M_i - m_i)(M_j - m_j).$$ 
\end{theorem}

\begin{proof}

We must consider both adding and removing a row from $X$.

Adding a row:\\
Let $X'_i = X_i \cup \{y_i\}$. Then, from Lemma \ref{cov:rewrite}, 
\begin{align}
\label{eq:addone}
\left\vert f_{ij}(X') - f_{ij}(X) \right\vert &= \left\vert n(\bar{X}_i - \bar{X}_i')(\bar{X}_j - \bar{X}_j') + (y_i - \bar{X}_i)(y_j - \bar{X}_j) \right\vert \hspace{1cm} \nonumber\\
	&\le  n \left\vert (\bar{X}_i - \bar{X}_i')(\bar{X}_j - \bar{X}_j') \right\vert + \left\vert (y_i - \bar{X}_i)(y_j - \bar{X}_j) \right\vert \nonumber\\
	&\le \frac{n}{(n+1)^2} (M_i - m_i)(M_j - m_j) + \frac{n^2}{(n+1)^2} (M_i - m_i)(M_j - m_j) \nonumber	\\
	& \hspace{8cm} \text{(By Lemmas \ref{lemma:term1bound} and \ref{lemma:term2bound})} \nonumber\\
	&= \frac{n}{n+1}  (M_i - m_i)(M_j - m_j).
\end{align}

Removing a row:\\
Let $Y$ be the last row of $X$, and let $X_i'  = X_i \setminus \{y_i\}$. Note that Lemma \ref{cov:rewrite} can be rewritten in this setting by parametrizing $n$ as $n-1$ and swapping X and X' in its expression:

$$ f_{ij}(X) = f_{ij}(X') + (n-1)(\bar{X}_i' - \bar{X}_i)(\bar{X}_j' - \bar{X}_j) + (y_i - \bar{X}_i')(y_j - \bar{X}_j'). $$

Lemmas \ref{lemma:term1bound} and \ref{lemma:term2bound} may be rewritten with the same reparametrization:
$$ (n-1)\left\vert (\bar{X}_i' - \bar{X}_i)(\bar{X}_j' - \bar{X}_j) \right\vert \le \frac{n-1}{n^2} (M_i - m_i)(M_j - m_j),$$
and 
$$ \left\vert (y_i - \bar{X}_i')(y_j - \bar{X}_j') \right\vert \le \frac{(n-1)^2}{n^2}(M_i - m_i)(M_j - m_j).$$
Then,
\begin{align}
\label{eq:subone}
\left\vert f_{ij}(X) - f_{ij}(X') \right\vert &= \left\vert (n-1)(\bar{X}_i' - \bar{X}_i)(\bar{X}_j' - \bar{X}_j) + (y_i - \bar{X}_i')(y_j - \bar{X}_j') \right\vert, \nonumber\\
	&\le  \left\vert (n-1)(\bar{X}_i' - \bar{X}_i)(\bar{X}_j' - \bar{X}_j) \right\vert + \left\vert(y_i - \bar{X}_i')(y_j - \bar{X}_j') \right\vert, \nonumber\\
	&\le \frac{n-1}{n^2} (M_i - m_i)(M_j - m_j) + \frac{(n-1)^2}{n^2}(M_i - m_i)(M_j - m_j), \nonumber\\
	&= \frac{n-1}{n} (M_i - m_i)(M_j - m_j).
\end{align}

Note that for any $n \ge 1$,
	\begin{equation}
	\label{ineq}
	 \frac{n}{n + 1} > \frac{n-1}{n}.
	\end{equation}

So, the worst-case bound always occurs in the ``add-one'' case, and in general the  $\ell_1$ sensitivity of $f(\cdot)$ is bounded by
 $$ \frac{n}{n+1}  (M_i - m_i)(M_j - m_j).$$ 
\end{proof}

\begin{corollary}
\label{cor:renorm}
Let $X \leftarrow \mathcal{X}$ where $\mathcal{X}_i$ is bounded above by $M_i$ and bounded below by $m_i$. Then the $\ell_1$-sensitivity in the add/drop-one model of the $ij^{\text{th}}$ element of the covariance matrix for $X$ is bounded above by 
$$ \frac{1}{n+1}(M_i - m_i)(M_j - m_j).$$
\end{corollary}

\begin{proof}
Note that the $ij^{\text{th}}$ element of the covariance matrix for $X$ is equal to $f(x)/n$.
\end{proof}

\begin{corollary}
\label{cor:renorm}
Let $X \leftarrow \mathcal{X}$ where $\mathcal{X}_i$ is bounded above by $M_i$ and bounded below by $m_i$. Then the $\ell_1$-sensitivity in the add/drop-one model of the $ij^{\text{th}}$ element of a sample covariance matrix for $X$ is bounded above by 
$$ \frac{n}{n^2-1}(M_i - m_i)(M_j - m_j).$$
\end{corollary}

\begin{proof}
Note that the $ij^{\text{th}}$ element of the sample covariance of $X$ is equal to $f(x)/(n-1)$, and that $(n-1)(n+1) = n^2 -1$.
\end{proof}

\subsection{$\ell_2$-sensitivity}

\begin{theorem}
Let $X \leftarrow \mathcal{X}$ where $\mathcal{X}_i$ is bounded above by $M_i$ and bounded below by $m_i$. Then the $\ell_2$-sensitivity in the add/drop-one model of the $ij^{\text{th}}$ element of the covariance matrix for $X$ is bounded above by
$$ \frac{1}{n+1}(M_i - m_i)(M_j - m_j).$$
\end{theorem}

\begin{proof}
% note you can also do this directly by squaring the bounds and bounding the central term. I can add that in but it's just a lot of cluttered arithmetic. 
This follows from the bounds in Equations \ref{eq:addone} and \ref{eq:subone} and the inequality in Equation \ref{ineq}, and a renormalization by $n$ from the definition of covariance.
\end{proof}

\begin{corollary}
\label{thm:l2addsub}
Let $X \leftarrow \mathcal{X}$ where $\mathcal{X}_i$ is bounded above by $M_i$ and bounded below by $m_i$. Then the $\ell_2$-sensitivity in the add/drop-one model of the $ij^{\text{th}}$ element of a sample covariance matrix for $X$ is bounded above by
$$\frac{n}{n^2-1}(M_i - m_i)(M_j - m_j).$$
\end{corollary}

\begin{proof}
The logic here is identical to the proof of Theorem \ref{thm:l2addsub}, with a renormalization by $n$ rather than by $n-1$.
\end{proof}

\section{Neighboring Definition: Change One}
\subsection{$\ell_1$-sensitivity}

\begin{theorem}
\label{thm:l1change1}
Let $X$ be a matrix of values and let $X_i$ indicate the $i^{\text{th}}$ column of the matrix. Let
$$ f_{ij} (X)= \sum_{k=1}^n (x_{ki} - \bar{X}_i)(x_{kj} - \bar{X}_j).$$
Say that the space of datapoints $\mathcal{X}_i$ that $X_i$ is drawn from is bounded above by $M_i$ and bounded below by $m_i$. Then the $\ell_1$-sensitivity in the change-one model of $f(\cdot)$ is bounded above by
$$ \frac{2(n-1)}{n}  (M_i - m_i)(M_j - m_j).$$
\end{theorem}

\begin{proof}
Recall from Equation \ref{eq:addone} that 
$$ \left\vert f_{ij}(X) - f_{ij}(X') \right\vert \le \frac{n}{(n+1)}  (M_i - m_i)(M_j - m_j).$$
and
$$ \left\vert f_{ij}(X) - f_{ij}(X'') \right\vert \le \frac{n}{(n+1)}  (M_i - m_i)(M_j - m_j).$$
Reparametrizing these equations so that $n$ is the size of $X'$ and $X''$ gives that 
$$ \left\vert f_{ij}(X) - f_{ij}(X') \right\vert \le \frac{n-1}{n}  (M_i - m_i)(M_j - m_j).$$
and
$$ \left\vert f_{ij}(X) - f_{ij}(X'') \right\vert \le \frac{n-1}{n}  (M_i - m_i)(M_j - m_j).$$
It then follows from the triangle inequality that 
$$ \left\vert f_{ij}(X') - f_{ij}(X'') \right\vert \le \frac{2(n-1)}{n}  (M_i - m_i)(M_j - m_j).$$
\end{proof}

\begin{corollary}
The $\ell_1$-sensitivity in the change-one model of covariance is bounded above by
$$\frac{2(n-1)}{n^2}  (M_i - m_i)(M_j - m_j).$$
\end{corollary}

\begin{proof}
Note that the $ij^{\text{th}}$ element of the covariance of $X$ is equal to $f(x)/n$.
\end{proof}

\begin{corollary}
\label{cor:renorm2}
The $\ell_1$-sensitivity in the change-one model of sample covariance is bounded above by
$$\frac{2}{n}  (M_i - m_i)(M_j - m_j).$$
\end{corollary}

\begin{proof}
Note that the $ij^{\text{th}}$ element of the sample covariance of $X$ is equal to $f(x)/(n-1)$.
\end{proof}

\subsection{$\ell_2$-sensitivity}

\begin{theorem}
\label{thm:l2change}
The $\ell_2$-sensitivity in the change-one model of covariance is bounded above by 
$$\frac{2(n-1)}{n^2}  (M_i - m_i)(M_j - m_j).$$
\end{theorem}

\begin{proof}
This follows from the bounds in the proof of Theorem \ref{thm:l1change1} and a renormalization by $n$.
\end{proof}

\begin{corollary}
The $\ell_2$-sensitivity in the change-one model of sample covariance is bounded above by 
$$\frac{2}{n}  (M_i - m_i)(M_j - m_j).$$
\end{corollary}

\begin{proof}
The logic here is identical to the proof of Theorem \ref{thm:l2change}, with a renormalization by $n-1$ rather than by $n$.
\end{proof}

\bibliographystyle{alpha}
\bibliography{mean}

\end{document}